# ğŸ§  RLHF Training Pipeline (From Scratch)

This project implements a **Reinforcement Learning with Human Feedback (RLHF)** pipeline, consisting of:

1. **Policy Model** â€” fine-tuned on instructions
2. **Reward Model** â€” learns to prefer better responses
3. **PPO Fine-tuning** â€” optimizes the policy with reward signals
4. **Testing** â€” to validate and chat with the final PPO model

Built entirely with **PyTorch** + **Hugging Face Transformers** ğŸš€

---

## ğŸ—‚ï¸ Folder Structure

```
RLHF Project/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Downloaded raw datasets (Yahma/Alpaca-Cleaned)
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ accepted_data.jsonl  # Human-approved (good) responses
â”‚   â”‚   â”œâ”€â”€ rejected_data.jsonl  # Human-rejected (bad) responses
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ policy/                  # Fine-tuned base model
â”‚   â”œâ”€â”€ reward/                  # Trained reward model checkpoint
â”‚   â”œâ”€â”€ ppo/                     # PPO fine-tuned model
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ policy_model.py          # Step 1: Train policy model
â”‚   â”œâ”€â”€ reward_model.py          # Step 2: Train reward model
â”‚   â”œâ”€â”€ ppo_model.py             # Step 3: PPO fine-tuning
â”‚   â”œâ”€â”€ test_ppo_model.py        # Step 4: Test PPO model
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .venv/
â””â”€â”€ README.md
```

---

## âš™ï¸ Environment Setup

### 1ï¸âƒ£ Create a Virtual Environment

```bash
python -m venv .venv
```

Activate it:

**Windows:**

```bash
.venv\Scripts\activate
```

**Linux/macOS:**

```bash
source .venv/bin/activate
```

---

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

**Example `requirements.txt`:**

```
torch
transformers
datasets
tqdm
```

---

## ğŸ§© Data Setup

Before running the training scripts, you must create the folders and download the dataset.

### 1ï¸âƒ£ Create Folders

```bash
mkdir -p data/raw data/processed models/policy models/reward models/ppo scripts
```

### 2ï¸âƒ£ Download Dataset (Yahma/Alpaca-Cleaned)

This dataset will serve as the base for policy and reward model training.

Make sure you have **Git LFS** installed:

```bash
git lfs install
```

Then, download the dataset into the `data/raw` directory:

```bash
cd data/raw
git clone https://huggingface.co/datasets/yahma/alpaca-cleaned
cd ../../
```

After this step, your `data/raw/alpaca-cleaned` folder will contain the original instructionâ€“response pairs.

---

## ğŸ§  RLHF Training Flow

### ï¿½\dfe2 Step 1: Train Policy Model

Fine-tune the base model (like GPT-2) on the Alpaca dataset.

```bash
python scripts/policy_model.py
```

â¡ï¸ Output: `models/policy/`

---

### ğŸŸ¡ Step 2: Train Reward Model

Train a DistilBERT-based reward model on **accepted vs rejected** responses.

```bash
python scripts/reward_model.py
```

â¡ï¸ Output: `models/reward/reward_model.pt`

---

### ğŸ”´ Step 3: PPO Fine-Tuning

Perform Proximal Policy Optimization (PPO) using the trained reward model.

```bash
python scripts/ppo_model.py
```

â¡ï¸ Output: `models/ppo/`

---

### ğŸ§ª Step 4: Test PPO Model

Interactively test or evaluate the fine-tuned PPO model.

```bash
python scripts/test_ppo_model.py
```

ğŸ§  Example Output:

![Model Output](assets/output.png)


---

## ğŸ§ª Optional: Run All Steps in Sequence

To automate the full RLHF flow:

```bash
python scripts/policy_model.py && \
python scripts/reward_model.py && \
python scripts/ppo_model.py && \
python scripts/test_ppo_model.py
```

---

## âš¡ GPU Check

Ensure CUDA is available before training:

```bash
python -c "import torch; print(torch.cuda.is_available())"
```

If `True`, GPU training is enabled âœ…

---

## ğŸ Summary

| Step | Script              | Description           | Output                          |
| ---- | ------------------- | --------------------- | ------------------------------- |
| 1ï¸âƒ£  | `policy_model.py`   | Fine-tunes base LLM   | `models/policy/`                |
| 2ï¸âƒ£  | `reward_model.py`   | Trains reward scorer  | `models/reward/reward_model.pt` |
| 3ï¸âƒ£  | `ppo_model.py`      | RLHF PPO fine-tuning  | `models/ppo/`                   |
| 4ï¸âƒ£  | `test_ppo_model.py` | Chat & test PPO model | Console output                  |

---

## â¤ï¸ Credits

Built using:

* [PyTorch](https://pytorch.org/)
* [Hugging Face Transformers](https://huggingface.co/transformers)
* [Yahma/Alpaca-Cleaned Dataset](https://huggingface.co/datasets/yahma/alpaca-cleaned)
* PPO and RLHF ideas inspired by [OpenAI InstructGPT](https://arxiv.org/abs/2203.02155)
