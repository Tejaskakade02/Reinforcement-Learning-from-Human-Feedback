ğŸ§  RLHF Training Pipeline (From Scratch)

This project implements a Reinforcement Learning with Human Feedback (RLHF) pipeline, consisting of:

Policy Model â€” fine-tuned on instructions

Reward Model â€” learns to prefer better responses

PPO Fine-tuning â€” optimizes the policy with reward signals

Testing â€” to validate and chat with the final PPO model

Built entirely with PyTorch + Hugging Face Transformers ğŸš€

ğŸ“‚ Folder Structure
RLHF Project/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Downloaded raw datasets (Yahma/Alpaca-Cleaned)
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ accepted_data.jsonl  # Human-approved (good) responses
â”‚   â”‚   â”œâ”€â”€ rejected_data.jsonl  # Human-rejected (bad) responses
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ policy/                  # Fine-tuned base model
â”‚   â”œâ”€â”€ reward/                  # Trained reward model checkpoint
â”‚   â”œâ”€â”€ ppo/                     # PPO fine-tuned model
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ policy_model.py          # Step 1: Train policy model
â”‚   â”œâ”€â”€ reward_model.py          # Step 2: Train reward model
â”‚   â”œâ”€â”€ ppo_model.py             # Step 3: PPO fine-tuning
â”‚   â”œâ”€â”€ test_ppo_model.py        # Step 4: Test PPO model
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .venv/
â””â”€â”€ README.md


âš™ï¸ Environment Setup
1ï¸âƒ£ Create a Virtual Environment
python -m venv .venv


Activate it:

Windows:

.venv\Scripts\activate


Linux/macOS:

source .venv/bin/activate

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt


Example requirements.txt:

torch
transformers
datasets
tqdm

ğŸ§© Data Setup

Before running the training scripts, you must create the folders and download the dataset.

1ï¸âƒ£ Create Folders
mkdir -p data/raw data/processed models/policy models/reward models/ppo scripts

2ï¸âƒ£ Download Dataset (Yahma/Alpaca-Cleaned)

This dataset will serve as the base for policy and reward model training.

Make sure you have Git LFS installed:

git lfs install


Then, download the dataset into the data/raw directory:

cd data/raw
git clone https://huggingface.co/datasets/yahma/alpaca-cleaned
cd ../../


After this step, your data/raw/alpaca-cleaned folder will contain the original instructionâ€“response pairs.

ğŸ§  RLHF Training Flow
ğŸŸ¢ Step 1: Train Policy Model

Fine-tune the base model (like GPT-2) on the Alpaca dataset.

python scripts/policy_model.py


â¡ï¸ Output: models/policy/

ğŸŸ¡ Step 2: Train Reward Model

Train a DistilBERT-based reward model on accepted vs rejected responses.

python scripts/reward_model.py


â¡ï¸ Output: models/reward/reward_model.pt

ğŸ”´ Step 3: PPO Fine-Tuning

Perform Proximal Policy Optimization (PPO) using the trained reward model.

python scripts/ppo_model.py


â¡ï¸ Output: models/ppo/

ğŸ§ª Step 4: Test PPO Model

Interactively test or evaluate the fine-tuned PPO model.

python scripts/test_ppo_model.py


ğŸ§  Example Output:

Prompt: Explain reinforcement learning simply.
Response: Reinforcement learning is when an AI learns from rewards and mistakes to make better choices.

ğŸ§° Optional: Run All Steps in Sequence

To automate the full RLHF flow:

python scripts/policy_model.py && \
python scripts/reward_model.py && \
python scripts/ppo_model.py && \
python scripts/test_ppo_model.py

âš¡ GPU Check

Ensure CUDA is available before training:

python -c "import torch; print(torch.cuda.is_available())"


If True, GPU training is enabled âœ…

ğŸ Summary
Step	Script	Description	Output
1ï¸âƒ£	policy_model.py	Fine-tunes base LLM	models/policy/
2ï¸âƒ£	reward_model.py	Trains reward scorer	models/reward/reward_model.pt
3ï¸âƒ£	ppo_model.py	RLHF PPO fine-tuning	models/ppo/
4ï¸âƒ£	test_ppo_model.py	Chat & test PPO model	Console output
â¤ï¸ Credits

Built using:

PyTorch

Hugging Face Transformers

Yahma/Alpaca-Cleaned Dataset

PPO and RLHF ideas inspired by OpenAI InstructGPT
